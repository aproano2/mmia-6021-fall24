{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "11f106f1-25f4-45f7-8b33-508c3e3edf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685e548-0dd0-4e1e-902d-6d65f62d9c88",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b3301f3-0566-44b4-b56b-90e4c5d00df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "83801587-6ec2-4696-bf81-9b566e635f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b557f909-feaf-40ff-8689-9ebe4c61cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'data/spa.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7953c2e0-6ef9-4537-a699-05263d7e3b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    try:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "            len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "            p[0].startswith(eng_prefixes)\n",
    "    except:\n",
    "        print(p)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0990a138-2adc-460b-98c5-69219b5ef364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(lang1, lang2, file):\n",
    "    text = open(file, encoding='utf-8').read().split('\\n')\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')][:2] for l in text ]\n",
    "    pairs = [pair for pair in pairs if len(pair) == 2]\n",
    "\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    \n",
    "    #pairs = filterPairs(pairs)\n",
    "    \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "f3c9c05a-7738-47eb-8862-abc397d1751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9305"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2b4746bf-31d7-4d38-8cb5-2fc0f63af872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141543"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "427df0d6-e5fe-44bf-9c82-b085fe7df36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 2992\n",
      "spa 4398\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4cffad1-383a-4581-91ac-4bcf76144ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 13789\n",
      "spa 26915\n"
     ]
    }
   ],
   "source": [
    "input_lang1, output_lang1, pairs1 = prepareData('eng', 'spa', file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97717e82-8474-4f11-b831-48b711130bd1",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "3556e239-ba60-4f63-bd91-d6ff6613daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b8344bc9-a0c6-41df-94f3-f515d46ee11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda656da-63bc-49df-b7e8-f97d8a7ffe3c",
   "metadata": {},
   "source": [
    "### Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ab0d887a-af8e-468c-b5dd-97af6c178fa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['i m up', 'estoy levantado'],\n",
       " ['i m tom', 'soy tom'],\n",
       " ['i m fat', 'estoy gordo'],\n",
       " ['i m fat', 'soy gorda'],\n",
       " ['i m fit', 'estoy en forma'],\n",
       " ['i m hit !', 'estoy afectado'],\n",
       " ['i m mad', 'estoy enojado'],\n",
       " ['i m mad', 'estoy enojada'],\n",
       " ['i m mad', 'estoy loco'],\n",
       " ['i m old', 'soy viejo'],\n",
       " ['i m shy', 'soy timido'],\n",
       " ['i m wet', 'estoy mojada'],\n",
       " ['i am old', 'estoy viejo'],\n",
       " ['i m back', 'he vuelto'],\n",
       " ['i m back', 'estoy de vuelta'],\n",
       " ['i m back', 'he vuelto !'],\n",
       " ['i m bald', 'soy calvo'],\n",
       " ['i m bald', 'estoy calvo'],\n",
       " ['i m calm', 'estoy calmado'],\n",
       " ['i m cool', 'estoy tranquilo']]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fa900112-47ad-46c7-b497-28728a4ae4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('eng', 'spa', file)\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids),\n",
    "                               torch.LongTensor(target_ids))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4991adda-01c6-4ac7-a6f9-14d605400e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9818e5-f1de-4b03-94c9-f6510b4ceef9",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c55784c4-a97f-4189-896f-d939625d2052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2d2982b-3cc5-4932-a8f1-f183b870f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "               print_every=100, plot_every=100):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
    "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "15e074af-59e2-45ad-a11a-fea298c9af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0ddf80e2-0acd-47b5-95b1-89e314afbae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counted words:\n",
      "eng 13789\n",
      "spa 26915\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (11,) into shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m----> 4\u001b[0m input_lang, output_lang, train_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(batch_size)\n\u001b[1;32m      6\u001b[0m encoder \u001b[38;5;241m=\u001b[39m EncoderRNN(input_lang\u001b[38;5;241m.\u001b[39mn_words, hidden_size)\n\u001b[1;32m      7\u001b[0m decoder \u001b[38;5;241m=\u001b[39m DecoderRNN(hidden_size, output_lang\u001b[38;5;241m.\u001b[39mn_words)\n",
      "Cell \u001b[0;32mIn[159], line 27\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m     tgt_ids\u001b[38;5;241m.\u001b[39mappend(EOS_token)\n\u001b[1;32m     26\u001b[0m     input_ids[idx, :\u001b[38;5;28mlen\u001b[39m(inp_ids)] \u001b[38;5;241m=\u001b[39m inp_ids\n\u001b[0;32m---> 27\u001b[0m     target_ids[idx, :\u001b[38;5;28mlen\u001b[39m(tgt_ids)] \u001b[38;5;241m=\u001b[39m tgt_ids\n\u001b[1;32m     29\u001b[0m train_data \u001b[38;5;241m=\u001b[39m TensorDataset(torch\u001b[38;5;241m.\u001b[39mLongTensor(input_ids),\n\u001b[1;32m     30\u001b[0m                            torch\u001b[38;5;241m.\u001b[39mLongTensor(target_ids))\n\u001b[1;32m     32\u001b[0m train_sampler \u001b[38;5;241m=\u001b[39m RandomSampler(train_data)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (11,) into shape (10,)"
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 80, print_every=5, plot_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5fde72-08dd-4142-9e77-f25b0abe5fdf",
   "metadata": {},
   "source": [
    "### Evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3ef04c46-62c8-442d-bdd6-04a1f7648efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "79977353-5c84-420c-abae-a4bdd281ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "778193a2-c167-4b75-9eac-6c0820d81895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> he s right behind you\n",
      "= el esta justo detras tuyo\n",
      "< el esta justo detras de ti en la amistad <EOS>\n",
      "\n",
      "> he is very kind to me\n",
      "= el es muy lindo conmigo\n",
      "< es muy amable conmigo <EOS>\n",
      "\n",
      "> we re done\n",
      "= terminamos\n",
      "< la tienda ya <EOS>\n",
      "\n",
      "> i m going back\n",
      "= voy de regreso\n",
      "< voy a estar con mi <EOS>\n",
      "\n",
      "> i m not a cook\n",
      "= no soy cocinero\n",
      "< no voy a tener una nina de terror <EOS>\n",
      "\n",
      "> i m in good health\n",
      "= gozo de buena salud\n",
      "< estoy seguro de que se esta preocupada <EOS>\n",
      "\n",
      "> she is all skin and bone\n",
      "= ella es piel y hueso\n",
      "< ella es piel y hueso y ella <EOS>\n",
      "\n",
      "> we re all tired\n",
      "= estamos todos cansados\n",
      "< estamos en nuestra luna de miel <EOS>\n",
      "\n",
      "> i m sure that he ll succeed\n",
      "= estoy seguro de que triunfara\n",
      "< estoy seguro de que va a dimitir <EOS>\n",
      "\n",
      "> i m ironing my handkerchiefs\n",
      "= estoy planchando mis panuelos\n",
      "< estoy planchando mis panuelos <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d4ee2030-6f5d-46a2-b17a-b5f2a1c6205e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['el', 'es', 'mi', 'hermano', 'menor', '<EOS>'], None)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " evaluate(encoder, decoder, 'he is my brother', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bd4bd4e1-6677-4a8d-a436-33daecd80550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['es', 'la', 'mujer', 'de', 'su', 'mujer', '<EOS>'], None)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'she is my sister', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4e4443fe-e664-4ba2-8131-bb889acbe130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['ella', 'es', 'atractiva', 'por', 'naturaleza', '<EOS>'], None)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'she is a teacher', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f09adbf7-98fd-4e0d-9e06-36c4407dd911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['somos', 'reunion', 'a', 'los', 'estados', 'unidos', '<EOS>'], None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'we are students', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5fb9f6cb-85e4-42bf-8673-83c54f9b154b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['soy', 'una', 'destacada', 'de', 'su', 'padre', '<EOS>'], None)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'i am a cook', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8b40dacf-9922-4ee6-91bd-12e7f46dcb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['estoy', 'planchando', 'mis', 'panuelos', '<EOS>'], None)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'i am cleaning my house', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "21c2d97f-5d61-4963-813d-68af5dfc6336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['el', 'es', 'un', 'estudiante', 'de', 'verdad', 'soy', 'profesor', '<EOS>'],\n",
       " None)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(encoder, decoder, 'is an idiot', input_lang, output_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c38772-0d0b-4ea4-ae38-4587f981d2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
